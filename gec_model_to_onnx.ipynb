{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHykCdlh6Kyf",
    "outputId": "ea993d93-94a8-4dff-f547-2fba144beb35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
      "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU transformers\n",
    "!pip install -qU tensorflow\n",
    "!pip install -qU onnx\n",
    "!pip install -qU onnxruntime\n",
    "!pip install -qU optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4F6w0oAVqvA",
    "outputId": "91f723ee-3443-4b51-9965-b719e400faf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-05 09:03:51.522478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743843831.841192    1738 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743843831.945644    1738 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743843832.623779    1738 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743843832.623832    1738 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743843832.623841    1738 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743843832.623845    1738 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-05 09:03:52.702261: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "usage: optimum-cli export onnx [-h] -m MODEL [--task TASK] [--opset OPSET] [--device DEVICE]\n",
      "                               [--fp16] [--dtype {fp32,fp16,bf16}] [--optimize {O1,O2,O3,O4}]\n",
      "                               [--monolith] [--no-post-process] [--variant VARIANT]\n",
      "                               [--framework {pt,tf}] [--atol ATOL] [--cache_dir CACHE_DIR]\n",
      "                               [--trust-remote-code] [--pad_token_id PAD_TOKEN_ID]\n",
      "                               [--library-name {transformers,diffusers,timm,sentence_transformers}]\n",
      "                               [--model-kwargs MODEL_KWARGS] [--legacy] [--no-dynamic-axes]\n",
      "                               [--no-constant-folding] [--batch_size BATCH_SIZE]\n",
      "                               [--sequence_length SEQUENCE_LENGTH] [--num_choices NUM_CHOICES]\n",
      "                               [--width WIDTH] [--height HEIGHT] [--num_channels NUM_CHANNELS]\n",
      "                               [--feature_size FEATURE_SIZE] [--nb_max_frames NB_MAX_FRAMES]\n",
      "                               [--audio_sequence_length AUDIO_SEQUENCE_LENGTH]\n",
      "                               [--point_batch_size POINT_BATCH_SIZE]\n",
      "                               [--nb_points_per_image NB_POINTS_PER_IMAGE]\n",
      "                               output\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "Required arguments:\n",
      "  -m MODEL, --model MODEL\n",
      "                        Model ID on huggingface.co or path on disk to load model from.\n",
      "  output                Path indicating the directory where to store the generated ONNX model.\n",
      "\n",
      "Optional arguments:\n",
      "  --task TASK           The task to export the model for. If not specified, the task will be auto-\n",
      "                        inferred based on the model. Available tasks depend on the model, but are\n",
      "                        among: ['audio-xvector', 'question-answering', 'zero-shot-object-\n",
      "                        detection', 'text-generation', 'zero-shot-image-classification', 'object-\n",
      "                        detection', 'mask-generation', 'reinforcement-learning', 'multiple-\n",
      "                        choice', 'text2text-generation', 'inpainting', 'image-to-text', 'audio-\n",
      "                        classification', 'depth-estimation', 'fill-mask', 'masked-im', 'token-\n",
      "                        classification', 'sentence-similarity', 'visual-question-answering',\n",
      "                        'audio-frame-classification', 'text-to-audio', 'feature-extraction',\n",
      "                        'automatic-speech-recognition', 'semantic-segmentation', 'text-to-image',\n",
      "                        'image-segmentation', 'text-classification', 'image-classification',\n",
      "                        'image-to-image']. For decoder models, use `xxx-with-past` to export the\n",
      "                        model using past key values in the decoder.\n",
      "  --opset OPSET         If specified, ONNX opset version to export the model with. Otherwise, the\n",
      "                        default opset for the given model architecture will be used.\n",
      "  --device DEVICE       The device to use to do the export. Defaults to \"cpu\".\n",
      "  --fp16                Use half precision during the export. PyTorch-only, requires `--device\n",
      "                        cuda`.\n",
      "  --dtype {fp32,fp16,bf16}\n",
      "                        The floating point precision to use for the export. Supported options:\n",
      "                        fp32 (float32), fp16 (float16), bf16 (bfloat16).\n",
      "  --optimize {O1,O2,O3,O4}\n",
      "                        Allows to run ONNX Runtime optimizations directly during the export. Some\n",
      "                        of these optimizations are specific to ONNX Runtime, and the resulting\n",
      "                        ONNX will not be usable with other runtime as OpenVINO or TensorRT.\n",
      "                        Possible options: - O1: Basic general optimizations - O2: Basic and\n",
      "                        extended general optimizations, transformers-specific fusions - O3: Same\n",
      "                        as O2 with GELU approximation - O4: Same as O3 with mixed precision (fp16,\n",
      "                        GPU-only, requires `--device cuda`)\n",
      "  --monolith            Forces to export the model as a single ONNX file. By default, the ONNX\n",
      "                        exporter may break the model in several ONNX files, for example for\n",
      "                        encoder-decoder models where the encoder should be run only once while the\n",
      "                        decoder is looped over.\n",
      "  --no-post-process     Allows to disable any post-processing done by default on the exported ONNX\n",
      "                        models. For example, the merging of decoder and decoder-with-past models\n",
      "                        into a single ONNX model file to reduce memory usage.\n",
      "  --variant VARIANT     Select a variant of the model to export.\n",
      "  --framework {pt,tf}   The framework to use for the ONNX export. If not provided, will attempt to\n",
      "                        use the local checkpoint's original framework or what is available in the\n",
      "                        environment.\n",
      "  --atol ATOL           If specified, the absolute difference tolerance when validating the model.\n",
      "                        Otherwise, the default atol for the model will be used.\n",
      "  --cache_dir CACHE_DIR\n",
      "                        Path indicating where to store cache.\n",
      "  --trust-remote-code   Allows to use custom code for the modeling hosted in the model repository.\n",
      "                        This option should only be set for repositories you trust and in which you\n",
      "                        have read the code, as it will execute on your local machine arbitrary\n",
      "                        code present in the model repository.\n",
      "  --pad_token_id PAD_TOKEN_ID\n",
      "                        This is needed by some models, for some tasks. If not provided, will\n",
      "                        attempt to use the tokenizer to guess it.\n",
      "  --library-name {transformers,diffusers,timm,sentence_transformers}\n",
      "                        The library on the model. If not provided, will attempt to infer the local\n",
      "                        checkpoint's library\n",
      "  --model-kwargs MODEL_KWARGS\n",
      "                        Any kwargs passed to the model forward, or used to customize the export\n",
      "                        for a given model.\n",
      "  --legacy              Export decoder only models in three files (without + with past and the\n",
      "                        resulting merged model).Also disable the use of position_ids for text-\n",
      "                        generation models that require it for batched generation. This argument is\n",
      "                        introduced for backward compatibility and will be removed in a future\n",
      "                        release of Optimum.\n",
      "  --no-dynamic-axes     Disable dynamic axes during ONNX export\n",
      "  --no-constant-folding\n",
      "                        PyTorch-only argument. Disables PyTorch ONNX export constant folding.\n",
      "\n",
      "Input shapes (if necessary, this allows to override the shapes of the input given to the ONNX exporter, that requires an example input).:\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Text tasks only. Batch size to use in the example input given to the ONNX\n",
      "                        export.\n",
      "  --sequence_length SEQUENCE_LENGTH\n",
      "                        Text tasks only. Sequence length to use in the example input given to the\n",
      "                        ONNX export.\n",
      "  --num_choices NUM_CHOICES\n",
      "                        Text tasks only. Num choices to use in the example input given to the ONNX\n",
      "                        export.\n",
      "  --width WIDTH         Image tasks only. Width to use in the example input given to the ONNX\n",
      "                        export.\n",
      "  --height HEIGHT       Image tasks only. Height to use in the example input given to the ONNX\n",
      "                        export.\n",
      "  --num_channels NUM_CHANNELS\n",
      "                        Image tasks only. Number of channels to use in the example input given to\n",
      "                        the ONNX export.\n",
      "  --feature_size FEATURE_SIZE\n",
      "                        Audio tasks only. Feature size to use in the example input given to the\n",
      "                        ONNX export.\n",
      "  --nb_max_frames NB_MAX_FRAMES\n",
      "                        Audio tasks only. Maximum number of frames to use in the example input\n",
      "                        given to the ONNX export.\n",
      "  --audio_sequence_length AUDIO_SEQUENCE_LENGTH\n",
      "                        Audio tasks only. Audio sequence length to use in the example input given\n",
      "                        to the ONNX export.\n",
      "  --point_batch_size POINT_BATCH_SIZE\n",
      "                        For Segment Anything. It corresponds to how many segmentation masks we\n",
      "                        want the model to predict per input point.\n",
      "  --nb_points_per_image NB_POINTS_PER_IMAGE\n",
      "                        For Segment Anything. It corresponds to the number of points per\n",
      "                        segmentation masks.\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2lH30pnA8CiH",
    "outputId": "a87d396d-27d3-4100-ca8c-3c69505751dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  my_trained_model(1).zip\n",
      "   creating: my_trained_model/\n",
      "  inflating: my_trained_model/spiece.model  \n",
      "  inflating: my_trained_model/training_args.bin  \n",
      "  inflating: my_trained_model/model.safetensors  \n",
      "  inflating: my_trained_model/tokenizer_config.json  \n",
      "  inflating: my_trained_model/config.json  \n",
      "  inflating: my_trained_model/generation_config.json  \n",
      "  inflating: my_trained_model/special_tokens_map.json  \n",
      "  inflating: my_trained_model/added_tokens.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip my_trained_model\\(1\\).zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "y4kP_CNRteYV",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e2df554a-1e41-48a8-e30a-57f74db23b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-05 11:07:08.584803: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743851229.006734   31246 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743851229.116093   31246 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743851229.971194   31246 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743851229.971290   31246 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743851229.971296   31246 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743851229.971306   31246 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-05 11:07:10.055153: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py:1318: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "Could not find ONNX initializer for torch parameter decoder.embed_tokens.weight. decoder.embed_tokens.weight will not be checked for deduplication.\n",
      "Could not find ONNX initializer for torch parameter encoder.embed_tokens.weight. encoder.embed_tokens.weight will not be checked for deduplication.\n",
      "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
      "\tdecoder.embed_tokens.weight: set() --> ignored (may be a parameter from a part of the model not exported)\n",
      "\tencoder.embed_tokens.weight: set() --> ignored (may be a parameter from a part of the model not exported)\n",
      "\tshared.weight: {'shared.weight'}\n",
      "\t\t-[x] values not close enough, max diff: 6.103515625e-05 (atol: 1e-05)\n",
      "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
      "- logits: max diff = 6.103515625e-05.\n",
      " The exported model was saved at: onnx_model\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx --model my_trained_model --task text2text-generation onnx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5PhSXIDBywK",
    "outputId": "52e6c72f-ee54-4099-f32f-c62f8220f0c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-05 11:08:37.416462: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743851317.442741   31626 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743851317.451084   31626 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743851317.472119   31626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743851317.472166   31626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743851317.472172   31626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743851317.472178   31626 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-05 11:08:37.478134: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "usage: optimum-cli onnxruntime quantize [-h] --onnx_model ONNX_MODEL -o OUTPUT [--per_channel]\n",
      "                                        (--arm64 | --avx2 | --avx512 | --avx512_vnni | --tensorrt | -c CONFIG)\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --arm64               Quantization for the ARM64 architecture.\n",
      "  --avx2                Quantization with AVX-2 instructions.\n",
      "  --avx512              Quantization with AVX-512 instructions.\n",
      "  --avx512_vnni         Quantization with AVX-512 and VNNI instructions.\n",
      "  --tensorrt            Quantization for NVIDIA TensorRT optimizer.\n",
      "  -c CONFIG, --config CONFIG\n",
      "                        `ORTConfig` file to use to optimize the model.\n",
      "\n",
      "Required arguments:\n",
      "  --onnx_model ONNX_MODEL\n",
      "                        Path to the repository where the ONNX models to quantize are located.\n",
      "  -o OUTPUT, --output OUTPUT\n",
      "                        Path to the directory where to store generated ONNX model.\n",
      "\n",
      "Optional arguments:\n",
      "  --per_channel         Compute the quantization parameters on a per-channel basis.\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli onnxruntime quantize --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TqXw-c44YRN",
    "outputId": "c6956707-8065-4e6e-a349-1e8de8e6b979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-05 11:10:57.512355: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743851457.568458   32198 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743851457.581006   32198 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743851457.611208   32198 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743851457.611275   32198 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743851457.611299   32198 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743851457.611308   32198 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-05 11:10:57.619982: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli onnxruntime quantize \\\n",
    "  --onnx_model onnx_model/ \\\n",
    "  --arm64 \\\n",
    "  --output quantized_onnx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9abRqthAOXBE"
   },
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "sess_encoder = ort.InferenceSession(\"onnx_model/encoder_model.onnx\", providers=['CPUExecutionProvider'])\n",
    "sess_decoder = ort.InferenceSession(\"onnx_model/decoder_model.onnx\", providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOHxiWL-cwAO"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"onnx_model\")\n",
    "\n",
    "input_text = \"correct: I am writing in order to express my disappointment about your musical show 'Over the Rainbow'.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"np\")\n",
    "input_ids = inputs[\"input_ids\"].astype(np.int64)\n",
    "attention_mask = inputs[\"attention_mask\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHKTPDPFixxN"
   },
   "outputs": [],
   "source": [
    "encoder_outputs = sess_encoder.run(\n",
    "    None,\n",
    "    {\"input_ids\": input_ids,\n",
    "     \"attention_mask\": attention_mask\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3tghe3QjZ89"
   },
   "outputs": [],
   "source": [
    "decoder_input_ids = np.array([[tokenizer.pad_token_id]], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hr52eaOlju81"
   },
   "outputs": [],
   "source": [
    "output_sequences = []\n",
    "for _ in range(50):  \n",
    "    decoder_outputs = sess_decoder.run(\n",
    "        None,\n",
    "        {\n",
    "            \"input_ids\": decoder_input_ids,\n",
    "            \"encoder_hidden_states\": encoder_outputs[0],  \n",
    "            \"encoder_attention_mask\": attention_mask\n",
    "        }\n",
    "    )\n",
    "    next_token_logits = decoder_outputs[0][0, -1, :]  \n",
    "    next_token = np.argmax(next_token_logits)  \n",
    "    output_sequences.append(next_token)\n",
    "    decoder_input_ids = np.concatenate([decoder_input_ids, [[next_token]]], axis=-1)\n",
    "    if next_token == tokenizer.eos_token_id:  \n",
    "        break\n",
    "\n",
    "output_text = tokenizer.decode(output_sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "EFEiNfermGim",
    "outputId": "456b31a9-8a9b-4c71-8dd7-7c68d13c5da0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"I am writing in order to express my disappointment with your musical show 'Over the Rainbow' .\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Opa_7jrtiPD",
    "outputId": "cbc30c7f-982f-47da-c6dc-034c4b4fc4f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py:1318: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/cache_utils.py:457: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/cache_utils.py:441: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  len(self.key_cache[layer_idx]) == 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('flan_t5_mobile_token/tokenizer_config.json',\n",
       " 'flan_t5_mobile_token/special_tokens_map.json',\n",
       " 'flan_t5_mobile_token/spiece.model',\n",
       " 'flan_t5_mobile_token/added_tokens.json',\n",
       " 'flan_t5_mobile_token/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "from optimum.exporters.onnx import main_export\n",
    "\n",
    "model_id = \"my_trained_model\"\n",
    "model = ORTModelForSeq2SeqLM.from_pretrained(model_id, export=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model.save_pretrained(\"flan_t5_mobile\")\n",
    "tokenizer.save_pretrained(\"flan_t5_mobile_token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YsfcxGPt91b",
    "outputId": "bb470209-399a-4527-aaa6-98ac61749047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/212.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU onnxruntime-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDGMGhlfuCmS",
    "outputId": "5f93137b-fd64-4e14-fe75-78d4c7753dbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=\"flan_t5_mobile/encoder_model.onnx\",\n",
    "    model_output=\"flan_t5_mobile/encoder_model_quant.onnx\",\n",
    "    weight_type=QuantType.QInt8\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
